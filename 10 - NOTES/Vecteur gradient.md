---
tags:
  - maths/algebre
---


---
***RÃ©fÃ©rences :***

---
## DÃ©finition

LeÂ **gradient**Â d'une fonction scalaire f : â„â¿ â†’ â„ est unÂ **vecteur**Â qui contient toutes les [[DÃ©rivÃ©e]] partielles de f.

### Notation mathÃ©matique

```
âˆ‡f(xâ‚, xâ‚‚, ..., xâ‚™) = grad f = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]áµ€
```

### Symboles utilisÃ©s

- **âˆ‡**Â (nabla) : opÃ©rateur gradient
- **grad f**Â : notation alternative
- **âˆ‚f/âˆ‚xáµ¢**Â : dÃ©rivÃ©e partielle par rapport Ã  xáµ¢

## Cas particuliers

### Fonction Ã  2 variables

```
f(x,y) â†’ âˆ‡f = [âˆ‚f/âˆ‚x]
              [âˆ‚f/âˆ‚y]
```

### Fonction Ã  3 variables

```
f(x,y,z) â†’ âˆ‡f = [âˆ‚f/âˆ‚x]
                [âˆ‚f/âˆ‚y]
                [âˆ‚f/âˆ‚z]
```

## InterprÃ©tation gÃ©omÃ©trique

### 1. Direction de plus forte croissance

- Le gradientÂ **pointe vers**Â la direction deÂ **plus forte augmentation**Â de f
- SaÂ **norme**Â ||âˆ‡f|| donne leÂ **taux de croissance maximal**

### 2. OrthogonalitÃ© aux courbes de niveau

- Le gradient estÂ **perpendiculaire**Â aux courbes de niveau (isolignes)
- En 3D : perpendiculaire aux surfaces de niveau

### 3. Visualisation

```
      âˆ‡f â†—
     /
â”€â”€â”€â”€â—â”€â”€â”€â”€ courbe de niveau f(x,y) = c
```

## PropriÃ©tÃ©s fondamentales

### LinÃ©aritÃ©

```
âˆ‡(Î±f + Î²g) = Î±âˆ‡f + Î²âˆ‡g
```

### RÃ¨gle du produit

```
âˆ‡(fg) = fâˆ‡g + gâˆ‡f
```

### RÃ¨gle de la chaÃ®ne

Si h(x) = f(g(x)), alors :

```
âˆ‡h = (âˆ‡f âˆ˜ g) Â· J_g
```

oÃ¹ J_g est la matrice jacobienne de g.

### Gradient d'une composition

```
âˆ‡f(g(x)) = (âˆ‡f)(g(x)) Â· âˆ‡g(x)
```

## Calcul pratique

### Exemple 1 : Fonction de [[forme quadratique]]

```
f(x,y) = xÂ² + 3yÂ² - 2xy + 5x

âˆ‡f = [âˆ‚f/âˆ‚x] = [2x - 2y + 5]
     [âˆ‚f/âˆ‚y]   [6y - 2x]
```

### Exemple 2 : Fonction exponentielle

```
f(x,y) = e^(xÂ²+yÂ²)

âˆ‡f = [2xe^(xÂ²+yÂ²)]
     [2ye^(xÂ²+yÂ²)]
```

### Exemple 3 : Fonction Ã  3 variables

```
f(x,y,z) = xyz + xÂ² - zÂ³

âˆ‡f = [yz + 2x ]
     [xz.     ]
     [xy - 3zÂ²]
```

## Applications majeures

### 1. Optimisation

**Condition nÃ©cessaire d'optimalitÃ© :**

- Point critique :Â **âˆ‡f = 0**
- Maximum local : âˆ‡f = 0 et [[Matrice hessienne]] dÃ©finie nÃ©gative
- Minimum local : âˆ‡f = 0 et [[Matrice hessienne]] dÃ©finie positive

### 2. Algorithmes de descente

**Descente de gradient :**

```
x_{k+1} = x_k - Î±âˆ‡f(x_k)
```

oÃ¹ Î± > 0 est le pas d'apprentissage.

### 3. Physique

- **Champ de forces**Â : F = -âˆ‡U (potentiel U)
- **Flux de chaleur**Â : proportionnel Ã  -âˆ‡T (tempÃ©rature T)
- **Champ Ã©lectrique**Â : E = -âˆ‡V (potentiel V)

### 4. Apprentissage automatique

- **RÃ©tropropagation**Â dans les rÃ©seaux de neurones
- **Optimisation**Â des fonctions de coÃ»t
- **RÃ©gression linÃ©aire**Â : minimisation par gradient

## Relation avec d'autres concepts

### DÃ©rivÃ©e directionnelle

Pour un vecteur unitaire u :

```
D_u f = âˆ‡f Â· u = ||âˆ‡f|| cos Î¸
```

oÃ¹ Î¸ est l'angle entre âˆ‡f et u.

### Matrice hessienne

```
H_f = âˆ‡(âˆ‡f) = [âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚  âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚‚  ...]
               [âˆ‚Â²f/âˆ‚xâ‚‚âˆ‚xâ‚  âˆ‚Â²f/âˆ‚xâ‚‚âˆ‚xâ‚‚  ...]
               [     â‹®           â‹®       â‹± ]
```

### Jacobienne

Pour f : â„â¿ â†’ â„áµ :

- Si m = 1 : Jacobienne = gradient transposÃ©
- Si m > 1 : Jacobienne = matrice des gradients

## Exemples d'interprÃ©tation

### GÃ©ographie

- **f(x,y)**Â = altitude au point (x,y)
- **âˆ‡f**Â = direction de plus forte pente
- **||âˆ‡f||**Â = inclinaison de la pente

### Ã‰conomie

- **f(x,y)**Â = profit en fonction de deux variables
- **âˆ‡f = 0**Â = optimum de profit
- **Direction de âˆ‡f**Â = comment ajuster les variables pour maximiser le profit

### Machine Learning

- **f(Î¸)**Â = fonction de coÃ»t (erreur)
- **âˆ‡f**Â = direction d'augmentation de l'erreur
- **-âˆ‡f**Â = direction pour rÃ©duire l'erreur (descente)

## Cas particuliers utiles

### Fonctions quadratiques

```
f(x) = Â½xáµ€Ax + báµ€x + c
âˆ‡f(x) = Ax + b
```

### Norme euclidienne au carrÃ©

```
f(x) = ||x||Â² = xáµ€x
âˆ‡f(x) = 2x
```

### Fonction log-vraisemblance

```
f(x) = log(g(x))
âˆ‡f(x) = âˆ‡g(x)/g(x)
```

## Remarques importantes

âš ï¸Â **Attention :**Â Le gradient n'existe que si toutes les dÃ©rivÃ©es partielles existent et sont continues.

ğŸ’¡Â **Intuition :**Â Le gradient "montre le chemin le plus raide vers le haut"

ğŸ¯Â **Application pratique :**Â En optimisation, on va dans la directionÂ **opposÃ©e**Â au gradient pour minimiser (descente de gradient)

âš¡Â **Vitesse :**Â La norme ||âˆ‡f|| indique Ã  quelle vitesse la fonction change dans la direction optimale

## FLASHCARD
TARGET DECK
Mathematics

START
Basic
This is a test 23444.
Back: $\frac{3}{4}$
TEST DE CONTINUITE SUR LIGNE 
<!--ID: 1759786136184-->
END

