---
MOC:
tags:
---
---
***R√©f√©rences :***

---
## Mesure de la covariance 

Elle permet de mettre en √©vidence le sens de la liaison et son intensit√© entre deux variables : 
$$ Cov(X,Y)= E(XY)-E(X)E(Y)$$

- $Cov(x,y) > 0$ : relation positive (x augmente, y augmente)
- $Cov(x,y) = 0$ : absence de relation monotone 
- $Cov(x,y) < 0$ : relation n√©gative (x augmente, y diminue)

> [!info]
> Si deux variables sont ind√©pendantes, alors la covariance est √©gale √† 0. 
> Mais ce n'est pas r√©ciproqie 


## Propri√©t√©s

- Sym√©trie : $Cov(x,y) = Cov(y,x)$
- Distibutivit√© : $Cov(x,y +z) = Cov(x,y) + Cov(x,z)$
- Covariance avec une constant est nulle : $Cov(X,a) = 0$
- Covariance avec variable transform√©e : $Cov(x,a +by) = bCov(x,y)$

## exemple

Soit X~N(0,1) et $Y=X^2$. 
Sont elles ind√©pendantes ? 

ici on est sur une [[Loi Normale]].
on a donc $Cov(X,Y) = Cov(X,X^2) = E[X.X^2]-E[X].E[X^2] = E[X^3]$
donc $E[X^3]=\int_{\infty}^{-\infty}x^3dx = 0$

## Correlations 

#### Pourquoi normaliser ?

**Probl√®me de la covariance :**

- D√©pend des unit√©s de mesure
- Difficile √† interpr√©ter : Cov(X,Y) = 150, c'est beaucoup ou peu ? ü§∑

**Solution = Corr√©lation :**

- Division par¬†œÉXœÉY¬†"nettoie" les unit√©s
- Ram√®ne toujours entre -1 et +1 ‚Üí interpr√©tation universelle


> [!tip] Analogie
>
> C'est comme la diff√©rence entre :
> 
> - **Covariance**¬†= dire "deux villes sont √† 500 km" (valeur absolue)
> - **Corr√©lation**¬†= dire "deux villes sont √† 50% de la distance max du pays" (valeur relative)

Cette normalisation est appliqu√©e gr√¢ce √† la [[Corr√©lation de pearson]]